name: Benchmark for PR
# This workflow runs benchmarks in parallel jobs with caching for performance.
# Base benchmarks use the upstream repository to ensure base revision exists.
# PR benchmarks use the fork repository for the head revision.
# Results are compared in a final job using artifacts for reliable data transfer.
on:
  pull_request:
    paths:
      - "crates/typstyle-core/**"
      - ".github/workflows/bench-pr.yml"
  workflow_dispatch:
    inputs:
      force_rerun:
        description: "Force re-run benchmarks even if cache exists"
        required: false
        default: false
        type: boolean

jobs:
  pre_job:
    permissions:
      actions: write
      contents: read
    name: Duplicate Actions Detection
    runs-on: ubuntu-latest
    outputs:
      should_skip: ${{ steps.skip_check.outputs.should_skip }}
    steps:
      - id: skip_check
        uses: fkirc/skip-duplicate-actions@v5
        with:
          cancel_others: "true"

  bench-base:
    name: Benchmark base branch
    uses: ./.github/workflows/bench-job.yml
    needs: pre_job
    if: needs.pre_job.outputs.should_skip != 'true'
    with:
      branch_type: base
      git_ref: ${{ github.event.pull_request.base.sha }}
      repository: ${{ github.event.pull_request.base.repo.full_name }}
      force_rerun: ${{ github.event.inputs.force_rerun || false }}

  bench-pr:
    name: Benchmark PR branch
    uses: ./.github/workflows/bench-job.yml
    needs: pre_job
    if: needs.pre_job.outputs.should_skip != 'true'
    with:
      branch_type: pr
      git_ref: ${{ github.event.pull_request.head.ref }}
      repository: ${{ github.event.pull_request.head.repo.full_name }}
      force_rerun: ${{ github.event.inputs.force_rerun || false }}

  compare:
    name: Generate benchmark comparison
    runs-on: ubuntu-latest
    needs: [bench-base, bench-pr]
    steps:
      - uses: actions/checkout@v4

      - name: Install critcmp
        uses: taiki-e/install-action@v2
        with:
          tool: critcmp

      - name: Download base benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-base
          path: base-results

      - name: Download PR benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-pr
          path: pr-results

      - name: Prepare benchmark data for comparison
        run: |
          mkdir -p target/criterion

          # Merge base and PR benchmark results
          cp -r base-results/* target/criterion/
          cp -r pr-results/* target/criterion/

          echo "Final benchmark structure:"
          ls -la target/criterion/

      - name: Generate comparison results
        run: |
          # Generate comparison results and save to file
          BENCH_RESULT=$(critcmp --color never base pr)

          # Save results to file for the comment workflow
          cat > benchmark-results.md << EOF
          ### ğŸ“Š Benchmark Performance Report

          \`\`\`console
          $BENCH_RESULT
          \`\`\`

          Generated by GitHub Actions on $(date)
          EOF

          echo "Generated benchmark comparison report"
          cat benchmark-results.md

      - name: Upload benchmark comparison
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison
          path: benchmark-results.md
          retention-days: 1
