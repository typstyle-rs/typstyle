name: Benchmark for PR
# This workflow runs benchmarks in parallel jobs with caching for performance.
# Base benchmarks use the upstream repository to ensure base revision exists.
# PR benchmarks use the fork repository for the head revision.
# Results are compared in a final job using artifacts for reliable data transfer.

permissions:
  contents: read
  actions: read
  pull-requests: read

on:
  pull_request:
    paths:
      - "crates/typstyle-core/**"
      - ".github/workflows/bench-pr.yml"
  workflow_dispatch:
    inputs:
      force_rerun:
        description: "Force re-run benchmarks even if cache exists"
        required: false
        default: false
        type: boolean

jobs:
  pre_job:
    permissions:
      actions: write
      contents: read
    name: Duplicate Actions Detection
    runs-on: ubuntu-latest
    outputs:
      should_skip: ${{ steps.skip_check.outputs.should_skip }}
    steps:
      - id: skip_check
        uses: fkirc/skip-duplicate-actions@v5
        with:
          cancel_others: "true"

  bench-base:
    name: Benchmark base branch
    uses: ./.github/workflows/bench-job.yml
    needs: pre_job
    if: needs.pre_job.outputs.should_skip != 'true'
    with:
      branch_type: base
      git_ref: ${{ github.event.pull_request.base.sha }}
      repository: ${{ github.event.pull_request.base.repo.full_name }}
      force_rerun: ${{ github.event.inputs.force_rerun || false }}

  bench-pr:
    name: Benchmark PR branch
    uses: ./.github/workflows/bench-job.yml
    needs: pre_job
    if: needs.pre_job.outputs.should_skip != 'true'
    with:
      branch_type: pr
      git_ref: ${{ github.event.pull_request.head.ref }}
      repository: ${{ github.event.pull_request.head.repo.full_name }}
      force_rerun: ${{ github.event.inputs.force_rerun || false }}

  compare:
    name: Generate benchmark comparison
    runs-on: ubuntu-latest
    needs: [bench-base, bench-pr]
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Nushell
        uses: hustcer/setup-nu@v3

      - name: Install critcmp
        uses: taiki-e/install-action@v2
        with:
          tool: critcmp

      - name: Download base benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-base
          path: base-results

      - name: Download PR benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-pr
          path: pr-results

      - name: Prepare benchmark data for comparison
        run: |
          mkdir -p target/criterion

          # Merge base and PR benchmark results
          cp -r base-results/* target/criterion/ 2>/dev/null || true
          cp -r pr-results/* target/criterion/ 2>/dev/null || true

          echo "Final benchmark structure:"
          ls -la target/criterion/

      - name: Generate comparison results
        run: |
          # Generate complete benchmark and binary size comparison report
          nu .github/scripts/generate-benchmark-report.nu base-results pr-results
          # python .github/scripts/generate-benchmark-report.py base-results pr-results

      - name: Save PR number
        run: |
          mkdir -p ./pr-info
          echo ${{ github.event.pull_request.number }} > ./pr-info/PR_NUMBER
          echo "Saved PR number: ${{ github.event.pull_request.number }}"

      - name: Upload benchmark comparison and PR info
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison
          path: |
            benchmark-results.md
            pr-info/
          retention-days: 7
