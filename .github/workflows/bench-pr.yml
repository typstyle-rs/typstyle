name: Benchmark for PR
# This workflow runs benchmarks in parallel jobs with rust-cache for persistence.
# Base and PR benchmarks run in parallel, then results are compared in a final job.
on:
  pull_request:
    paths:
      - "crates/typstyle-core/**"
      - ".github/workflows/bench-pr.yml"
  workflow_dispatch:
    inputs:
      force_rerun:
        description: 'Force re-run benchmarks even if cache exists'
        required: false
        default: false
        type: boolean

jobs:
  pre_job:
    permissions:
      actions: write
      contents: read
    name: Duplicate Actions Detection
    runs-on: ubuntu-latest
    outputs:
      should_skip: ${{ steps.skip_check.outputs.should_skip }}
    steps:
      - id: skip_check
        uses: fkirc/skip-duplicate-actions@v5
        with:
          cancel_others: "true"

  bench-base:
    name: Benchmark base branch
    runs-on: ubuntu-latest
    needs: pre_job
    if: needs.pre_job.outputs.should_skip != 'true'
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.base.sha }}
          token: ${{ secrets.GITHUB_TOKEN }}

      # Check cache first before installing Rust
      - name: Cache criterion benchmark data
        uses: actions/cache@v4
        id: bench-cache
        with:
          path: target/criterion
          key: criterion-base-${{ github.event.pull_request.base.sha }}
          restore-keys: |
            criterion-base-

      - name: Check for cached benchmark
        id: check-cache
        run: |
          if [ "${{ steps.bench-cache.outputs.cache-hit }}" == "true" ] && [ "${{ github.event.inputs.force_rerun }}" != "true" ]; then
            echo "Found cached benchmark results and not forcing rerun"
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            if [ "${{ github.event.inputs.force_rerun }}" == "true" ]; then
              echo "Forcing benchmark rerun (cache ignored)"
            else
              echo "No cached benchmark results found"
            fi
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      # Only install Rust and dependencies if we need to run benchmarks
      - uses: dtolnay/rust-toolchain@stable
        if: steps.check-cache.outputs.cache-hit != 'true'

      - uses: Swatinem/rust-cache@v2
        if: steps.check-cache.outputs.cache-hit != 'true'

      - name: Run base benchmark
        if: steps.check-cache.outputs.cache-hit != 'true'
        run: |
          echo "Running base branch benchmark..."
          cargo bench --workspace -- --save-baseline base

      # Always upload artifacts for reliable cross-job data transfer
      - name: Upload base benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-base-${{ github.event.pull_request.base.sha }}
          path: target/criterion
          retention-days: 7

  bench-pr:
    name: Benchmark PR branch
    runs-on: ubuntu-latest
    needs: pre_job
    if: needs.pre_job.outputs.should_skip != 'true'
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          token: ${{ secrets.GITHUB_TOKEN }}

      # Check cache first before installing Rust
      - name: Cache criterion benchmark data
        uses: actions/cache@v4
        id: bench-cache
        with:
          path: target/criterion
          key: criterion-pr-${{ github.event.pull_request.head.sha }}
          restore-keys: |
            criterion-pr-

      - name: Check for cached benchmark
        id: check-cache
        run: |
          if [ "${{ steps.bench-cache.outputs.cache-hit }}" == "true" ] && [ "${{ github.event.inputs.force_rerun }}" != "true" ]; then
            echo "Found cached benchmark results and not forcing rerun"
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            if [ "${{ github.event.inputs.force_rerun }}" == "true" ]; then
              echo "Forcing benchmark rerun (cache ignored)"
            else
              echo "No cached benchmark results found"
            fi
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      # Only install Rust and dependencies if we need to run benchmarks
      - uses: dtolnay/rust-toolchain@stable
        if: steps.check-cache.outputs.cache-hit != 'true'

      - uses: Swatinem/rust-cache@v2
        if: steps.check-cache.outputs.cache-hit != 'true'

      - name: Run PR benchmark
        if: steps.check-cache.outputs.cache-hit != 'true'
        run: |
          echo "Running PR branch benchmark..."
          cargo bench --workspace -- --save-baseline pr

      # Always upload artifacts for reliable cross-job data transfer
      - name: Upload PR benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-pr-${{ github.event.pull_request.head.sha }}
          path: target/criterion
          retention-days: 7

  compare:
    name: Compare and comment benchmark results
    runs-on: ubuntu-latest
    needs: [bench-base, bench-pr]
    permissions:
      pull-requests: write
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable

      - name: Install critcmp
        uses: taiki-e/install-action@v2
        with:
          tool: critcmp

      - name: Download base benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-base-${{ github.event.pull_request.base.sha }}
          path: base-results

      - name: Download PR benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-pr-${{ github.event.pull_request.head.sha }}
          path: pr-results

      - name: Prepare benchmark data for comparison
        run: |
          mkdir -p target/criterion

          # Merge base and PR benchmark results
          cp -r base-results/* target/criterion/
          cp -r pr-results/* target/criterion/

          echo "Final benchmark structure:"
          ls -la target/criterion/

      - name: Compare and comment results
        run: |
          # Generate comparison results
          BENCH_RESULT=$(critcmp --color never base pr)

          # Generate comment content
          echo "BENCHMARK_COMMENT<<EOF" >> $GITHUB_ENV
          echo "### ðŸ“Š Benchmark Performance Report" >> $GITHUB_ENV
          echo "" >> $GITHUB_ENV
          echo '```console' >> $GITHUB_ENV
          echo "$BENCH_RESULT" >> $GITHUB_ENV
          echo '```' >> $GITHUB_ENV
          echo "" >> $GITHUB_ENV
          echo "Generated by GitHub Actions on $(date)" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

          echo "$BENCH_RESULT"

      - name: Find Comment
        uses: peter-evans/find-comment@v3
        id: fc
        with:
          issue-number: ${{ github.event.pull_request.number }}
          comment-author: "github-actions[bot]"
          body-includes: "### ðŸ“Š Benchmark Performance Report"

      - name: Create or update comment
        uses: peter-evans/create-or-update-comment@v4
        continue-on-error: true
        with:
          comment-id: ${{ steps.fc.outputs.comment-id }}
          issue-number: ${{ github.event.pull_request.number }}
          body: ${{ env.BENCHMARK_COMMENT }}
          edit-mode: replace
