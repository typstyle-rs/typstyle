name: Reusable Benchmark Job
# Reusable workflow for running benchmarks with caching

permissions:
  contents: read
  actions: read

on:
  workflow_call:
    inputs:
      branch_type:
        description: "Type of branch (base or pr)"
        required: true
        type: string
      git_ref:
        description: "Git reference to checkout"
        required: true
        type: string
      repository:
        description: "Repository to checkout from"
        required: false
        type: string
        default: ""
      force_rerun:
        description: "Force re-run benchmarks even if cache exists"
        required: false
        type: boolean
        default: false

jobs:
  benchmark:
    name: Benchmark ${{ inputs.branch_type }} branch
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4
        with:
          repository: ${{ inputs.repository || github.repository }}
          ref: ${{ inputs.git_ref }}
          token: ${{ secrets.GITHUB_TOKEN }}

      # Generate cache key from code content for smart caching
      - name: Generate cache key from code content
        id: cache-key
        run: |
          # Use GitHub's built-in hashFiles function for simpler and more reliable hashing
          CONTENT_HASH="${{ hashFiles('crates/typstyle-core/**/*.rs', 'Cargo.lock') }}"
          echo "content-hash=${CONTENT_HASH:0:16}" >> $GITHUB_OUTPUT
          echo "Generated content hash: ${CONTENT_HASH:0:16}"

      # Cache criterion benchmark data using content-based keys
      - name: Cache criterion benchmark data
        uses: actions/cache@v4
        id: bench-cache
        with:
          path: |
            target/criterion
            target/bloat
          key: criterion-${{ inputs.branch_type }}-${{ steps.cache-key.outputs.content-hash }}
          # No restore-keys - benchmarks must match exact content for accuracy

      - name: Check for cached benchmark
        id: check-cache
        run: |
          # Check cache status and validate cached data
          if [ "${{ steps.bench-cache.outputs.cache-hit }}" == "true" ] && [ "${{ inputs.force_rerun }}" != "true" ]; then
            # Validate that cached data is complete and valid
            if [ -d "target/criterion" ] && [ -n "$(ls -A target/criterion 2>/dev/null)" ] && [ -f "target/bloat/bloat-${{ inputs.branch_type }}.json" ] && [ -f "target/bloat/size-${{ inputs.branch_type }}.bytes" ]; then
              echo "Found valid cached benchmark and bloat results"
              echo "Using cache from key: ${{ steps.bench-cache.outputs.cache-matched-key }}"
              echo "cache-hit=true" >> $GITHUB_OUTPUT
            else
              echo "Cache hit but data incomplete, will re-run benchmark"
              echo "cache-hit=false" >> $GITHUB_OUTPUT
            fi
          else
            if [ "${{ inputs.force_rerun }}" == "true" ]; then
              echo "Forcing benchmark rerun (cache ignored)"
            else
              echo "No cached benchmark results found"
            fi
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      # Only install Rust and dependencies if we need to run benchmarks
      - uses: dtolnay/rust-toolchain@stable
        if: steps.check-cache.outputs.cache-hit != 'true'

      - name: Install cargo-bloat
        if: steps.check-cache.outputs.cache-hit != 'true'
        uses: taiki-e/install-action@v2
        with:
          tool: cargo-bloat

      - uses: Swatinem/rust-cache@v2
        if: steps.check-cache.outputs.cache-hit != 'true'

      - name: Run benchmark
        if: steps.check-cache.outputs.cache-hit != 'true'
        run: |
          echo "Running ${{ inputs.branch_type }} branch benchmark..."
          cargo bench --workspace -- --save-baseline ${{ inputs.branch_type }}

      - name: Build release binary and collect bloat data
        if: steps.check-cache.outputs.cache-hit != 'true'
        run: |
          echo "Building release binary for ${{ inputs.branch_type }} branch..."
          BINARY_NAME="typstyle"
          cargo build --release --bin $BINARY_NAME

          if [ -f "target/release/$BINARY_NAME" ]; then
            # Create bloat data directory
            mkdir -p target/bloat

            # Get binary size and save for comparison
            BINARY_SIZE=$(stat -c%s target/release/$BINARY_NAME 2>/dev/null || stat -f%z target/release/$BINARY_NAME)
            BINARY_SIZE_HUMAN=$(numfmt --to=iec-i --suffix=B $BINARY_SIZE)
            echo "$BINARY_SIZE" > target/bloat/size-${{ inputs.branch_type }}.bytes

            # Generate cargo-bloat report in JSON format
            cargo bloat --release --crates --bin $BINARY_NAME --message-format json > target/bloat/bloat-${{ inputs.branch_type }}.json 2>/dev/null || echo "{}" > target/bloat/bloat-${{ inputs.branch_type }}.json

            echo "Binary size: $BINARY_SIZE_HUMAN ($BINARY_SIZE bytes)"
            echo "Cargo-bloat analysis completed"
          else
            echo "Warning: Release binary not found at target/release/$BINARY_NAME"
          fi

      # Always upload artifacts for reliable cross-job data transfer
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-${{ inputs.branch_type }}
          path: |
            target/criterion
            target/bloat
          retention-days: 1
          if-no-files-found: error
